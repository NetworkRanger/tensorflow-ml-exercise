# 自然语言处理

# 7.1 文本处理介绍
到目前为止，我们介绍的机器学习算法都是应用到数值输入。如果我们想应用到文本处理，就必须找到一种方法将文本转化成数字。有许多方法可以实现该功能，这里我们介绍一些常用的方法。

假设要处理一句话，TensorFlow让这类机器学习变得简单，我们可以把单词转换成易观察的有序的数字。一句话转换成1 2 3 4 5。然后当我们看到一个新的句子，机器学习很容易将其翻译成3 4 0 5，用0代表不能识别的单词。在前面的两个例子中，我们把词汇限制为6个数字。对于长文本，我们可以根据所需要选择单词数，一般选择的单词是使用频率最高的词汇，并用0标其他任意单词。

如果单词learning用数字4表示，单词makes用数字2表示，那么很容易地认为单词learning是单词makes的两倍。因为我们不期望两个单词之间有数值关系，所以我们假设这些数字仅仅代表分类，而不具有数值关系。

另一个问题是，两句话和长度不一样。我们的算法模型期望的输入是相同长度的语句。为了解决这个问题，我们把句子转换成一个稀疏向量。该稀疏向量的规则是：如果对应索引上的单词存在，则对应的索引位置的值为1。

TensorFlow|maeks|machine|learning|easy
-----------|-----|-------|--------|-----
1|2|3|4|5

first_sentence = [0,1,1,1,1,1]

Machine|learning|is|easy
--------|--------|---|-----
3|4|0|5

second_sentence = [1,0,0,1,1,1]

这种方法的缺点是损失了语句中单词顺序的特征。TensorFlow makes machine learning easy和machine learning makes TensorFlow easy, 不同的两句话却具有相同句子稀疏向量。

值得注意的是，这些向量的长度是相等的，并且与我们所选的词汇一致。一般情况下，我们会选择非常大的词汇量，所以这些句子向量非常稀疏。刚介绍的这种词嵌入方法称为"词袋"。

另一个缺点是，单词is和TensorFlow具有相同的数值化索引值1。但是我们很明显地看到单词is要比单词TensorFlow的重要性弱。

# 7.2 词袋的使用

## 1. 导入必要的编程库。本例中需要.zip文件库来解压从UCI机器学习数据库中下载的.zip文件
## 2. 为了让脚本运行时不用每次都去下载文件数据，我们将下载文件存储，并检查之间是否保存过。该步骤避免了文本数据的重复下载。下载完文本数据集后，抽取输入数据和目标数据，并调整目标值(垃圾短信(spam)置为1，正常短信(ham)置为0)。
## 3. 为了减小词汇量大小，我们对文本进行规则化处理。移除文本中大小写和数字的影响
## 4. 计算最长句子大小。我们使用文本数据集和文本长度直方图,并取最佳截止点
## 5. TensorFlow 自带分词器VocabularyProcessor()，该函数位于learn.preprocessing库
## 6. 分割数据集为训练集和测试集
## 7. 声明词嵌入矩阵。将句子单词转成索引，再将索引转成one-hot向量，该向量为单位矩阵。我们使用该矩阵为每个单词查找稀疏向量
## 8. 因为最后要进行逻辑回归预测垃圾短信的概率，所以我们需要声明逻辑回归向量。然后声明占位符，注意x_data输入占位符是整数类型，因为它被用来查找单位矩阵和行索引，而TensorFlow要求其为整数类型
## 9. 使用TensorFlow的嵌入查找函数来映射句子中的单词为单位矩阵的one-host向量。然后把前面的词向量求和
## 10. 有了每个句子的固定长度的句子向量之后，我们进行逻辑回归训练。声明逻辑回归算法模型。因为一次做一个数据点的随机训练，所有扩展输入数据的维度，并进行线性回归操作。记住，TensorFlow中的损失函数已经包含了sigmoid激励函数，所以我们不需要在输出时加入激励函数
## 11. 声明训练模型的损失函数、预测函数和优化器
## 12. 接下来初始化计算图中的变量
## 13. 开始迭代训练。TensorFlow的内建函数vocab_processor.fit()是一个符合本例的生成器。我们将使用该函数来进行随机训练逻辑回归模型。为了得到准确度的趋势，我们保留最近50次迭代的平均值。如果只绘制当前值，我们会依赖预测训练数据点是否正确而得到1或者0的值
## 14. 训练结果如下
## 15. 为了得到测试集的准确度，我们重复处理过程，对测试文本只进行预测操作，而不进行训练操作


# 7.3 用TensorFlow实现TF-IDF算法

## 1. 导入必要的编程库。本例中会导入scikit-learn的TF-IDF处理模块处理文本数据集
## 2. 创建一个计算图会话, 声明批量大小和词汇的最大长度
## 3. 加载文本数据集。可以从网站下载或者从上次保存的temp文件夹加载
## 4. 声明词汇大小。本例中也会将所有字符转换成小写，剔除标点符号和数字
## 5. 为了使用scikit-learn的TF—IDF处理函数，我们需要输入切分好的语句(即将句子切分为相关的单词）。nltk包可以提供非常棒的分词器来实现分词功能
## 6. 分割数据集为训练集和测试集
## 7. 声明逻辑回归模型的变量和数据集的占位符
## 8. 声明算法模型操作和损失函数。注意，逻辑回归算法的sigmoid部分是在损失函数中实现的
## 9. 为计算图增加预测和准确度函数(可以让我们看到模型训练中训练集和测试集的准确度)
## 10. 声明优化器，初始化计算图中的变量
## 11. 遍历迭代训练模型10000次，记录测试集和训练集损失，以及每迭代100次的准确度，然后每迭代500次就打印状态信息
## 12. 输出结果如下
## 13. 绘制训练集和测试的准确度的损失函数