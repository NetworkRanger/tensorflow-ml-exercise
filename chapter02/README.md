# TensorFlow 进阶

## [2.2 计算图中的操作](./demo_2.2.py)

1. 首先，声明张量和占位符。这里，创建一个numpy数组，传入计算图操作

## [2.3 TensorFlow 的嵌入Layer](./demo_2.3.py)

1. 首先，创建数据和占位符
2. 接着，创建矩阵乘法和加法中要用到的常量矩阵
3. 现在声明操作，表示成计算图
4. 最后，通过计算图赋值

## [2.4 TensorFlow 的多层Layer](./demo_2.4.py)

1. 首先，通过numpy创建2D图像，4x4像素图片。
2. 下面在计算图中创建占位符
3. 为了创建过滤4x4像素图片的滑动窗口，我们将用TensorFlow内建函数conv2d()(常用来做图像处理) 卷积2x2形状的常量窗口
4. 注意，我们通过conv2d()函数的name参数，把这层Layer命名为"Moving_Avg_Window"
5. 现在定义一个自定义Layer,操作滑动窗口平均的2x2的返回值。
6. 现在把刚刚新定义的Layer加入到计算图中，并且用tf.name_scope()命名唯一的Layer名字，后续在计算图中可折叠/扩展Custom_Layer层
7. 为占位符传入4x4像素图片，然后执行计算图

## [2.5 TensorFlow实现损失函数](./demo_2.5.py)

1. L2正则损失函数（即欧拉损失函数）
2. L1正则损失函数（即绝对值损失函数）
3.  Pseudo-Huber损失函数是Huber损失函数的连续、平滑估计，试图利用L1和L2正则消减极值处的陡峭，使得目标值附近连续
4. 分类损失函数是用来评估预测分类结果的
5. 重新给x_vals和target赋值,保存返回值并在下节绘制出来
6. Hinge损失函数主要用来评估支持向量机算法，但有时也用来评估神经网络算法
7. 两类交叉熵损失函数(Cross-entropy loss)有时也作为逻辑损失函数
8. Sigmoid交叉熵损失函数(Sigmoid cross entropy loss)与上一个损失函数非常类似，有一点不同的是，它先把x_vals值通过sigmoid函数转换，再计算交叉熵损失
9. 加权交叉熵损失函数(Weighted cross entropy loss)是Sigmoid交叉熵损失函数的加权，对正目标加权
10. Softmax交叉熵损失函数(Softmax cross-entropy loss)是作用于非归一化的输出结果,只针对单个目标分类的计算损失
11. 稀疏Softmax交叉损失函数（Spare softmax cross-entropy loss)和上一个损失函数类似，它是把目标分类为true的转换成index，而Softmax交叉熵损失函数将目标转换成概率分布

### 损失函数

损失函数|使用类型|优点|缺点
-------|-------|---|---
L2|回归算法|更稳定|缺少健壮
L1|回归算法|更健壮|缺省稳定
Psuedo-Huber|回归算法|更健壮、稳定|参数多
Hinge|分类算法|常用于SVM的最大距离|异常值导致无边界损失
Cross-entropy|分类算法|更稳定|缺少健壮，出现无边界损失

### 模型评价指标

模型指标|描述
-------|---
R平方值(R-squared)|对简单的线性模型来讲，用于度量因变量的变异中可由自变量解释部分所占的比例
RMSE(平均方差)|对连续模型来讲，平均方差是度量预测的值和观察的值之差的样本标准差
混淆矩阵(Confusion matrix)|对分类模型来讲，以矩阵形式将数据集中的记录按照真实的类别与分别模型预测的分类判断两个标准进行分析汇总，其每一列代表预测值，每一行代表的是实际的类别。理想情况下，混淆矩阵是对角矩阵
召回率(Recall)|对于分类模型来讲，召回率是正类预测为正类数与所有预测正类的比值
精准度(Precision)|对于分类模型来讲，精准度是正类预测为正类数与所有实际正类数的比值
F值(F-score)|对于分类模型来讲，F值是召回率和精准度的调和平均数

## 2.6 [TensorFlow 实现反向传播](./demo_2.6.py)

1. 导入Python的数值计算模块，numpy和tensorflow
2. 创建计算图会话
3. 生成数据，创建占位符和变量A
4. 增加乘法操作
5. 增加L2正则损失函数
6. 在运行之前，需要初始化变量
7. 现在声明变量的优化器。
8. 最后一步是训练算法。
9. 现在将介绍简单的分类算法例子
10.首先，重置计算图，并且重新初始化变量
11. 从正态分布(N(-1,1), N(3,1))生成数据
12. 增加转换操作
13. 由于指定的损失函数期望批量数的维度，这里使用expand_dims()函数增加维度
14. 初始化变量A
15. 声明损失函数，这里使用一个带非归一化logits的交叉熵的损失函数，同时会用sigmod函数转换
16. 如前面回归算法的例子，增加一个优化器函数让TensorFlow知道如何更新和偏差变量
17. 最后，通过随机选择的数据迭代几百次，相应地更新变量A

### 训练类型

训练类型|优点|缺点
-------|----|---
随机训练|脱离局部最小|一般需更多次迭代才收敛
批量训练|快速得到最小损失|耗费更多计算资源

## 2.7 [TensorFlow 实现随机训练和批量训练](./demo_2.7.py)

1. 开始声明批量大小
2. 接下来，声明模型的数据、占位符和变量
3. 现在在计算图中增加矩阵乘法操作，切记矩阵乘法不满足交换律，所以在matmul()函数中的矩阵参数顺序要正确
4. 改变损失函数
5. 声明优化器
6. 在训练中通过循环迭代优化模型算法
7. 迭代100次输出最终返回值

## [2.8 TensorFlow 实现创建张量](./demo_2.8.py)

1. 导入相应的工具库，初始化计算图
2. 导入iris数据集，根据目标数据是否为山鸢尾将其转换成1或者0。由于iris数据集将山鸢尾标记为0，我们将其从0置为1，同时把其他物种标记为0
3. 声明变量训练大小，数据占位符和模型变量
4. 定义线性模型
5. 增加TensorFlow 的 sigmoid 交叉熵损失函数 sigmoid_cross_entropy_with_logits()
6. 声明优化器方法，最小化交叉熵损失
7. 创建一个变量初始化操作，然后让TensorFlow执行它
8. 现在迭代100次训练线性模型
9. 下面的命令抽取模型变量并绘图

## [2.9 TensorFlow 实现模型评估](./demo_2.9.py)

1. 加载所需的编程库，创建计算图、数据集、变量和占位符
2. 声明算法模型、损失函数和优化器算法
3. 像以往一样迭代训练模型
4. 现在，为了评估训练模型，将打印训练数据集和测试数据集训练所有的 MSE 损失函数值
5. 对于分类模型的例子，与前面的例子类似。创建准确率函数(accuracy function), 分别调用sigmoid来测试分类是否正确
6. 重新加载计算图，创建数据集、变量和占位符
7. 在计算图中，增加模型和损失函数，初始化变量，并创建优化器
8. 现在进行迭代训练
9. 为了评估训练模型，我们创建预测操作
10. 模型训练结果，比如准确度、MSE等，将帮助我们评估机器学习模型