# 基于TensorFlow 的线性回归

## 3.1 线性回归介绍

线性回归算法是统计分析、机器学习和科学计算中最重要的算法之一，也是最常使用的算法之一，所以需要理解其是如何实现的，以及线性回归算法的各种优点。相对于许多其他算法来讲，线性回归算法是最易解释的。以每个特征的数值直接代表该特征对目标值或者因变量的影响。
线性回归算法是统计分析、机器学习和科学计算中最重要的算法之一，也是最常使用的算法之一，所以需要理解其是如何实现的，以及线性回归算法的各种优点。相对于许多其他算法来讲，线性回归算法是最易解释的。以每个特征的数值直接代表该特征对目标值或者因变量的影响。

## [3.2 用TensorFlow 求逆矩阵](./demo_3.2.py)

1. 导入必要的编程库，初始化计算图，并生成数据
2. 创建后续求逆方法所需的矩阵
3. 将A和矩阵转换成张量
4. 现在，使用TensorFlow的tf.matrix_inverse()方法
5. 从解中抽取系数、斜率和y截距y-intercept

## [3.3 用TensorFlow 实现矩阵分解](./demo_3.3.py)

1. 导入编程库，初始化计算图，生成数据集
2. 找到方阵的Cholesky矩阵分解，A.T*A
3. 抽取系数

## [3.4 用TensorFlow 实现线性回归算法](./demo_3.4.py)

1. 导入必要的编程库，创建计算图，加载数据集
2. 声明学习率，批量大小，占位符和模型变量
3. 增加线性模型，y=Ax+b
4. 下一步，声明L2损失函数，其为批量损失的平均值
5. 现在遍历迭代，并在随机选择的批量数据上进行模型训练
6. 抽取系数，创建最佳拟合直线
7. 这里将线绘制两幅图

## [3.5 理解线性回归中的损失函数](./demo_3.5.py)

1. 除了损失函数外，程序的开始与以往一样，导入必要的编程库，创建一个会话，加载 数据集，创建占位符，定义变量和模型
2. 损失函数改为L1正则损失函数
3. 现在继续初始化变量

## [3.6 用TensorFlow 实现戴明回归算法](./demo_3.6.py)

1. 导入必要的编程库，创建计算图，加载数据集
2. 损失函数是由分子和分母组成的几何公式。给你直线y = mx + b,点(x0,y0)到直线的距离公式为d=abs(y0-(mx0+b))/sqrt(m**2+1)
3. 现在初始化变量，声明优化器，遍历迭代训练集以得到参数
4. 绘制输出结果

## [3.7 用TensorFlow 实现lasso 回归和岭回归算法](./demo_3.7.py)

1. 这次还是使用iris数据集，导入必要的编程库，创建一个计算图会话，加载数据集，声明批量大小，创建占位符，变量和模型输出
2. 增加损失函数，其为改良过的连续阶跃函数，lasso回归的截止点设为0.9。这里意味着斜率系统不超过0.9
3. 初始化变量和声明优化器
4. 遍历迭代支行一段时间，因为需要一会才会收敛。最后结果显示斜率系数小于0.9
5. 绘制输出结果

## [3.8 用TensorFlow 实现弹性网络回归算法](./demo_3.5.py)

1. 导入必要的编程库并初始化一个计算图
2. 加载数据集
3. 声明批量大小、占位符、变量和模型输出
4. 对于弹性网络回归算法，损失函数包含斜率L1正则和L2正则。创建L1和L2正则项，然后加入到损失函数中
5. 现在初始化变量，声明优化器，然后遍历迭代支行，训练拟合得到系数
6. 代码支行输出的结果
7. 现在能观察到，随着训练迭代后损失函数已收敛

## [3.9 用TensorFlow实现逻辑回归算法](./demo_3.5.py)

1. 导入必要的编程库，包括requests模块，加为我们将通过超链接访问低出生体重数据集。初始化一个计算图
2. 通过requests模块加载数据集，指定要傅使用的特征。实际出生体特征和ID两列不需要
3. 分割数据集为测试集和训练集
4. 将所有特征缩放到0和1区间(min-max缩放), 逻辑回归收敛的效果更好。下面将归一化特征
5. 声明批量大小、占位符、变量和逻辑模型。这步不需要用sigmoid函数将封装输出结果，因为sigmoid操作是包含在内建损失函数 中的
6. 声明损失函数，其包含sigmoid函数。初始化变量，声明优化器
7. 除记录损失函数外，也需要记录分类器在训练集和测试集上的准确度。所以创建一个预测准确度的预测函数
8. 开始遍历迭代训练，记录损失值和准确度
9. 绘制损失和精确度