# 神经网络算法

# 6.1 神经网络算法基础

神经网络算法在识别图像和语音、识别手写、解理文本、图像分割、对话系统、自动驾驶等领域不断打破记录。神经网络算法是一种简单易实现的、很重要的机器学习算法。

神经网络算法的概念已出现几十年，但是它仅仅在最近由于计算能力(计算处理、算法效率和数据集大小) 的提升能训练大规模网络才获得新的发展。

神经网络算法是对输入数据矩阵进行一系列的基本操作。这些操作通常包括非线性函数的加法和乘法。逻辑回归算法是斜率与特征点积求和后进行非线性sigmoid函数计算。神经网络算法表达形式更通用，允许任意形式的基本操作和非线性函数的结合，包括绝对值、最大值、最小值等。

神经网络算法的一个重要的"黑科技"是"反向传播"。反向传播是一种基于学习率和损失函数返回值来更新模型变量的过程。

神经网络算法另外一个重要的特性是非线性激励函数。因为大部分神经网络算法仅仅是加法操作和乘法操作的结合，所以它们不能进行非线性数据样本集的模型训练。为了解决该问题，我们在神经网络算法中使用非线性激励函数，这将使得神经网络算法能够解决大部分非线性的问题。

记住，如前面见过的大部分算法，神经网络算法对所选择的超参数是敏感的。我们将看到不同的学习率、损失函数和优化对模型训练的影响。

>>> 学习资料
>>> * 用程序的方式介绍神经网络算法 https://karpathy.github.io/neuralnets/
>>> * 深度学习笔记  https://randomekek.github.io/deep/deeplearning.html

# 6.2 使用TensorFlow实现门函数

## 1. 加载TensorFlow模块，创建一个计算图会话
## 2. 声明模型变量、输入数据集和占位符。本例输入数据为5，所以乘法因为为10，可以得到50的预期值(5*10=10)
## 3. 增加操作到计算图中
## 4. 声明损失函数：输出结果与预期目标值(50)之间的L2距离函数
## 5. 初始化模型变量，声明标准梯度下降优化算法
## 6. 优化模型输出结果。连续输入值5， 反向传播损失函数来更新模型变量以达到值10
## 7. 输出结果如下
## 8. 对两个嵌套操作的例子f(x) = a*x+b, 也执行上述相同的步骤
## 9. 开始第二个例子，不同在于本例中包含两个模型变量： a和b
## 10. 优化模型变量，训练输出结果，以达到预期目标值50
## 11. 输出结果如下

# 6.3 使用门函数和激励函数

## 1. 导入必要的编程库，初始化一个计算图会话。对于学习在TensorFlow中如何设置随机种子而言，这也是一个很好的例子。这里将使用TensorFlow和Numpy模块和随机数生成器。对于相同的随机种子集，我们应该能够复现
## 2. 声明批量大小、模型变量、数据集和占位符。在计算图中为两个相似的神经网络模型(仅激励函数不同)传入正态分布数据
## 3. 声明两个训练模型，即sigmoid激励模型和ReLU激励模型
## 4. 损失函数都采用模型输出和预期值0.75之间的差值的L2范数平均
## 5. 声明优化算法
## 6. 遍历迭代训练模型，每个模型迭代750次。保存损失函数输出和激励函数的返回值，以便后续绘图
## 7. 下面是绘制损失函数和激励函数的代码

* 激励函数比较

激励函数|优点|缺点
-------|----|----
Sigmoid激励函数|输出的极值很少|收敛太慢
ReLU激励函数|快速收敛|返回结果中容易出现极值

# 6.4 用TensorFlow实现单层神经网络

## 1. 创建计算图会话，导入必要的编程库
## 2. 加载Iris数据集，存储花萼长度作为目标值，然后开始一个计算图会话
## 3. 因为数据集比较小，我们设置一个种子使得返回结果可复现
## 4. 为了准备数据集，我们创建一个80-20分的训练集和测试集。通过min-max缩放法正则化特征值为0到1之间
## 5. 现在为数据集和目标值声明批量大小和占位符
## 6. 这一步相当重要，声明有合适形状的模型变量。我们能声明隐藏层为任意大小，本例中设置为有五个隐藏节点
## 7. 分两步声明训练模型: 第一步，创建一个隐藏层输出；第二步，创建训练模型的最后输出
## 8. 这里定义均方误差为损失函数
## 9. 声明优化算法，初始化模型变量
## 10. 遍历迭代训练模型。我们也初始化两个列表(list)存储训练损失和测试损失。在每次迭代训练时，随机选择批量训练数据来拟合模型
## 11. 使用matplotlib绘制损失函数

# 6.5 用TensorFlow实现神经网络常见层

## 1. 导入需要的编程库，创建计算图会话
## 2. 初始化数据，该数据为NumPy数组，长度为25。创建传入数据的占位符
## 3. 定义一个卷积层的函数。接着声明一个随机过滤层，创建一个卷积层
## 4. TensorFlow的激励函数默认是逐个元素进行操作。这意味着，在部分层中使用激励函数。下面创建一个激励函数并初始化
## 5. 声明一个池化层函数，该函数在一维向量的移动窗口上创建池化层函数。对于本例，其宽度为5
## 6. 最后一层连接的是全连接层。创建一个函数，该函数输入一维数据，输出值的索引。记住一维数组做矩阵乘法需要提前扩展为二维
## 7. 初始化所有的变量，运行计算图打印出每层的输出结果
## 8. 输出结果如下

# 6.6 用TensorFlow实现多层神经网络

## 1. 导入必要的编程库
## 2. 使用requests模块从网站加载数据集，然后分离出需要的特征数据和目标值
## 3. 为了后面可以复现，为NumPy和TensorFlow设置随机种子，然后声明批量大小
## 4. 分割数据集为80-20的训练集和测试集，然后使用min-max方法归一化输入特征数据为0到1之间
## 5. 因为有多个层含有相似的变量初始化，因此我们将创建一个初始化函数，该函数可以初始化加权权重和偏置
## 6. 初始化占位符。本例中将有八个输入特征数据和一个输出结果(出生体重，单位：克)
## 7. 全连接层将在三个隐藏层中使用三次，为了避免代码上的重复，我们将创建一个层函数来初始化算法模型
## 8. 现在创建算法模型。对于每一层(包括输出层），我们将初始化一个权重矩阵、偏置矩阵和全连接层。在本例中，三个隐藏层的大小分别为25、10和3
## 9. 使用L1范数损失函数(绝对值), 声明优化器(Adam优化器）和初始化变量
## 10. 迭代训练模型200次。下面的代码也包括存储训练损失和测试损失，选择随机批量大小和每25次迭代就打印状态
## 11. 输出结果如下
## 12. 使用matplotlib模块绘制训练损失和测试损失代码
## 13. 现在我们想比较预测出体重结果和前面的逻辑结果。逻辑回归算法中，我们在迭代上千次后得到了大约60%的精确度。为了在这里做比较，我们将输出训练集/测试集和回归结果，然后传入一个指示函数(判断是否大于2500克),将回归结果转换成分类结果
## 14. 准确度的结果如下

# 6.7 线性预测模型的优化

## 1. 导入必要的编程库，初始化计算图会话
## 2. 加载低出生体重数据集，并对其进行抽取和归一化。有一点不同的是，本例中将使用低出生体重指示变量作为目标值，而不是实际出生体重
## 3. 声明批量大小和占位符
## 4. 我们声明函数来初始化算法模型中的变量和层。为了创建一个更好的逻辑层，我们需要创建一个返回输入层的逻辑层的函数。换句话说，我们需要使用全连接 层，返回每层的值。注意，损失函数包括最终的sigmoid函数，所以我们指定最后一层不必返回输出的sigmoid值
## 5. 声明神经网络的三层(两个隐藏层和一个输出层)。我们为每层初始化一个权重矩阵和偏置矩阵，并定义每层的操作
## 6. 声明损失函数(本例中使用的是交叉熵损失函数)和优化算法，并初始化变量
## 7. 为了评估和比较算法模型，创建计算图预测操作和准确操作。这使得我们可以传入测试集并计算准确度
## 8. 准备开始遍历迭代训练模型。本例训练 1500次，并为后续绘图保存模型的损失函数和训练集/测试集准确度
## 9. 输出结果如下
## 10. 下面的代码块展示如何用matplotlib模块绘制交叉熵损失函数和测试集/训练集准确度

# 6.7 线性预测模型的优化

## 1. 导入必要的编程库，初始化计算图会话
## 2. 加载低出生体重数据集，并对其进行抽取和归一化。有一点不同的是，本例中将使用低出生体重指示变量作为目标值，而不是实际出生体重
## 3. 声明批量大小和占位符
## 4. 我们声明函数来初始化算法模型中的变量和层。为了创建一个更好的逻辑层，我们需要创建一个返回输入层的逻辑层的函数。换句话说，我们需要使用全连接 层，返回每层的值。注意，损失函数包括最终的sigmoid函数，所以我们指定最后一层不必返回输出的sigmoid值
## 5. 声明神经网络的三层(两个隐藏层和一个输出层)。我们为每层初始化一个权重矩阵和偏置矩阵，并定义每层的操作
## 6. 声明损失函数(本例中使用的是交叉熵损失函数)和优化算法，并初始化变量
## 7. 为了评估和比较算法模型，创建计算图预测操作和准确操作。这使得我们可以传入测试集并计算准确度
## 8. 准备开始遍历迭代训练模型。本例训练 1500次，并为后续绘图保存模型的损失函数和训练集/测试集准确度
## 9. 输出结果如下
## 10. 下面的代码块展示如何用matplotlib模块绘制交叉熵损失函数和测试集/训练集准确度

# 6.8 用TensorFlow基于神经网络实现井字棋

## 1. 导入必要的编程库
## 2. 声明训练模型的批量大小
## 3. 为了让棋盘看起来更清楚，我们创建一个井字棋的打印函数
## 5. 棋盘位置列表和对应的最佳落子点数据位于.csv文件中。我们将创建get_moves_from_csv()函数来加载文件中的棋盘和最佳落子点数据，并保存成元组
## 6. 创建一个get_rand_move()函数，返回一个随机变换棋盘和落子点
## 7. 初始化计算图会话，加载数据集文件，创建训练集
## 8. 前面提到，我们将从训练集中移除一个棋盘位置和对应的最佳落子点，来看训练的模型是否可以生成最佳走棋。下面棋盘的最佳落子点是棋盘位置索引为6的位置
## 9. 创建init_weights()函数和model()函数，分别实现初始化模型变量和模型操作。注意，模型中并没有包含softmax()激励函数，因为softmax()激励函数会在损失函数中出现
## 10. 声明占位符、变量和模型
## 11. 声明算法模型的损失函数，该函数是最后输出的逻辑变换的平均softmax值。然后声明训练步长和优化器。为了将来可以和训练好的模型对局，我们也需要创建预测操作
## 12. 初始化变量，遍历迭代训练神经网络模型
## 13. 绘制模型训练的损失函数

# 测试模型

## 1. 为了测试模型，将展示如何在测试棋盘(从训练集中移除的数据)使用。我们希望看到模型能生成预测落子点的索引，并且索引值为6。在大部分情况下，模型都会成功预测
## 2. 输出结果如下
## 3. 为了能够评估训练模型，我们计划和训练好的模型进行对局。为了实现该功能，我们创建一个函数来检测是否赢了棋局，这样程序才能在该结束的时间喊停
## 4. 现在遍历迭代，同训练模型进行对局。起始棋盘为空棋盘，即为全0值；然后询问棋手要在哪个位置落棋子，即输入0-8的索引值；接着将其传入训练模型进行预测。对于模型的走棋，我们获得了多个可能的预测。最后显示井字棋游戏的样例。对于该游戏来说，我们发现训练的模型表现得并不理想
## 5. 人机交互的输出结果如下